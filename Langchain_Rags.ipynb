{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8807286,"sourceType":"datasetVersion","datasetId":5296986},{"sourceId":4302,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3097}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-29T08:06:38.627442Z","iopub.status.idle":"2024-06-29T08:06:38.627789Z","shell.execute_reply.started":"2024-06-29T08:06:38.627604Z","shell.execute_reply":"2024-06-29T08:06:38.627636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install langchain-community langchain --quiet\n!pip install unstructured chromadb tiktoken --quiet\n!pip install -q torch transformers accelerate bitsandbytes transformers sentence-transformers --q","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:45:53.859903Z","iopub.execute_input":"2024-06-29T11:45:53.860445Z","iopub.status.idle":"2024-06-29T11:47:20.233157Z","shell.execute_reply.started":"2024-06-29T11:45:53.860415Z","shell.execute_reply":"2024-06-29T11:47:20.231960Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 30.1.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"Importing the necessary libraries","metadata":{}},{"cell_type":"code","source":"from langchain.document_loaders import UnstructuredPDFLoader\nfrom langchain.indexes import VectorstoreIndexCreator\nimport os\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:47:20.235457Z","iopub.execute_input":"2024-06-29T11:47:20.236214Z","iopub.status.idle":"2024-06-29T11:47:25.713181Z","shell.execute_reply.started":"2024-06-29T11:47:20.236174Z","shell.execute_reply":"2024-06-29T11:47:25.712458Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from langchain_community.document_loaders import DirectoryLoader","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:47:25.714356Z","iopub.execute_input":"2024-06-29T11:47:25.714875Z","iopub.status.idle":"2024-06-29T11:47:25.722211Z","shell.execute_reply.started":"2024-06-29T11:47:25.714840Z","shell.execute_reply":"2024-06-29T11:47:25.721388Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Loading the data to Langchain loader. The DirectoryLoader is used, since the complete text files are present in a directory.","metadata":{}},{"cell_type":"code","source":"loader = DirectoryLoader(\"/kaggle/input/rags-txt-docs/Rags_txt_file\", glob=\"**/*.txt\")\ndocs = loader.load()\nlen(docs)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:08:13.638164Z","iopub.execute_input":"2024-06-29T08:08:13.638561Z","iopub.status.idle":"2024-06-29T08:12:42.879855Z","shell.execute_reply.started":"2024-06-29T08:08:13.638530Z","shell.execute_reply":"2024-06-29T08:12:42.878953Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"50"},"metadata":{}}]},{"cell_type":"code","source":"print(docs[0].page_content[:1000])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:12:42.880969Z","iopub.execute_input":"2024-06-29T08:12:42.881459Z","iopub.status.idle":"2024-06-29T08:12:42.886187Z","shell.execute_reply.started":"2024-06-29T08:12:42.881435Z","shell.execute_reply":"2024-06-29T08:12:42.885283Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Mistral 7B\n\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\n\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\n\nWilliam El Sayed\n\nAbstract\n\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\n\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\n\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\n\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\n\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\n\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\n\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\n\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n\nautomated benc\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install langchain-chroma --q","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:47:25.724095Z","iopub.execute_input":"2024-06-29T11:47:25.724678Z","iopub.status.idle":"2024-06-29T11:47:39.764563Z","shell.execute_reply.started":"2024-06-29T11:47:25.724653Z","shell.execute_reply":"2024-06-29T11:47:39.763630Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Error parsing requirements for nest-asyncio: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/nest_asyncio-1.5.8.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"The text splitter is used to split the loaded documents. This is because we want to create the embedding vectors of these documents, but most of the embedding models can handle embeddings of smaller dimensions. Thus, we create chunks of these documents, and allow small overlap of these chunks so that the semantic relation is preserved to some extent.","metadata":{}},{"cell_type":"code","source":"splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=30)\n\nchunked_docs = splitter.split_documents(docs)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:12:56.471956Z","iopub.execute_input":"2024-06-29T08:12:56.472341Z","iopub.status.idle":"2024-06-29T08:12:57.207902Z","shell.execute_reply.started":"2024-06-29T08:12:56.472297Z","shell.execute_reply":"2024-06-29T08:12:57.207055Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"embeddings = HuggingFaceEmbeddings(model_name=\"avsolatorio/GIST-Embedding-v0\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:47:39.765879Z","iopub.execute_input":"2024-06-29T11:47:39.766152Z","iopub.status.idle":"2024-06-29T11:47:58.575783Z","shell.execute_reply.started":"2024-06-29T11:47:39.766126Z","shell.execute_reply":"2024-06-29T11:47:58.574705Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n  warn_deprecated(\n2024-06-29 11:47:44.148589: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-29 11:47:44.148716: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-29 11:47:44.273305: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76dae69427d64763b1027a4a5f4fd995"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1958eb9643554a3495e47f302ff78064"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/117k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f40c992b9a04404a5d3c5e663b7fe11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d36898160afd423d9023ef2ef9b44a8d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5af198aa2c1f4f22aba138e1d44f8938"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6c9a99fa6fb495da5b05a706cd5a39d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4288fd571dce4dd38c1663a349bac882"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd3f78ad83514490b333b26fa3df45e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7348b6d32fa424db9a737c9cbd14bae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74074c1a06db446db8c6f09a76f2115e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eceaa7351f704cb38910dfc4ac4f01fe"}},"metadata":{}}]},{"cell_type":"markdown","source":"The splitted chunks are then embedded using hugging face embeddings and then stored in chroma vetor store.","metadata":{}},{"cell_type":"code","source":"# import\nfrom langchain_chroma import Chroma\n\ndb = Chroma.from_documents(chunked_docs, embeddings, collection_name=\"chroma_embeddings\", persist_directory=\"./chroma_embedding_db\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T11:53:24.652366Z","iopub.execute_input":"2024-06-29T11:53:24.653296Z","iopub.status.idle":"2024-06-29T11:53:24.657236Z","shell.execute_reply.started":"2024-06-29T11:53:24.653236Z","shell.execute_reply":"2024-06-29T11:53:24.656309Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Based on **similarity score**, top 7 embedding vectors are retreived using the retreiver from the vector store.","metadata":{}},{"cell_type":"code","source":"vector_retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 7})","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:16:00.773744Z","iopub.execute_input":"2024-06-29T08:16:00.774010Z","iopub.status.idle":"2024-06-29T08:16:00.778459Z","shell.execute_reply.started":"2024-06-29T08:16:00.773986Z","shell.execute_reply":"2024-06-29T08:16:00.777488Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def pretty_print_docs(docs):\n    print(\n        f\"\\n{'-' * 100}\\n\".join(\n            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:16:00.779390Z","iopub.execute_input":"2024-06-29T08:16:00.779646Z","iopub.status.idle":"2024-06-29T08:16:00.787428Z","shell.execute_reply.started":"2024-06-29T08:16:00.779599Z","shell.execute_reply":"2024-06-29T08:16:00.786666Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# docs = vector_retriever.get_relevant_documents(query=\"What are the layers in multi head attention?\")\n# pretty_print_docs(docs)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:09:01.971660Z","iopub.execute_input":"2024-06-29T04:09:01.972295Z","iopub.status.idle":"2024-06-29T04:09:01.975849Z","shell.execute_reply.started":"2024-06-29T04:09:01.972265Z","shell.execute_reply":"2024-06-29T04:09:01.974948Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from langchain.llms import HuggingFaceHub\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.prompts import PromptTemplate\nfrom dotenv import load_dotenv","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:16:00.788574Z","iopub.execute_input":"2024-06-29T08:16:00.788919Z","iopub.status.idle":"2024-06-29T08:16:00.817354Z","shell.execute_reply.started":"2024-06-29T08:16:00.788888Z","shell.execute_reply":"2024-06-29T08:16:00.816672Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import os\nfrom getpass import getpass\n\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass(\"Enter HuggingFace Hub Token:\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:16:00.825883Z","iopub.execute_input":"2024-06-29T08:16:00.826148Z","iopub.status.idle":"2024-06-29T08:16:05.653511Z","shell.execute_reply.started":"2024-06-29T08:16:00.826114Z","shell.execute_reply":"2024-06-29T08:16:05.652748Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter HuggingFace Hub Token: ·····································\n"}]},{"cell_type":"code","source":"import getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:16:05.654606Z","iopub.execute_input":"2024-06-29T08:16:05.654962Z","iopub.status.idle":"2024-06-29T08:16:17.385966Z","shell.execute_reply.started":"2024-06-29T08:16:05.654930Z","shell.execute_reply":"2024-06-29T08:16:17.385242Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdin","text":" ········································\n"}]},{"cell_type":"code","source":"from langchain_cohere import ChatCohere\nfrom langchain_core.messages import HumanMessage","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:16:41.348744Z","iopub.execute_input":"2024-06-29T08:16:41.349571Z","iopub.status.idle":"2024-06-29T08:16:41.592409Z","shell.execute_reply.started":"2024-06-29T08:16:41.349531Z","shell.execute_reply":"2024-06-29T08:16:41.591463Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"chat = ChatCohere(model=\"command\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:16:41.593480Z","iopub.execute_input":"2024-06-29T08:16:41.594113Z","iopub.status.idle":"2024-06-29T08:16:41.622266Z","shell.execute_reply.started":"2024-06-29T08:16:41.594086Z","shell.execute_reply":"2024-06-29T08:16:41.621601Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"The **LLMChainExtractor** extracts key information from documents using a language model, identifying sections most relevant to the user's query. The **ContextualCompressionRetriever** combines a base retriever, which searches for relevant documents, and refines these documents by extracting the most pertinent parts. The **EmbeddingsFilter** enhances this process by using embeddings to filter documents, selecting those that are semantically similar to the query. Together, these steps improve the RAG system by focusing on the most relevant information, enhancing both the accuracy and efficiency of responses, and ensuring the language model works with high-quality, contextually relevant data.","metadata":{}},{"cell_type":"code","source":"from langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\n#creating the compressor\ncompressor = LLMChainExtractor.from_llm(llm=chat)\n\n#compressor retriver = base retriever + compressor\ncompression_retriever = ContextualCompressionRetriever(base_retriever=vector_retriever,\n                                                       base_compressor=compressor)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:16:41.623858Z","iopub.execute_input":"2024-06-29T08:16:41.624118Z","iopub.status.idle":"2024-06-29T08:16:42.109119Z","shell.execute_reply.started":"2024-06-29T08:16:41.624096Z","shell.execute_reply":"2024-06-29T08:16:42.108119Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from getpass import getpass\nimport os\n# from langchain.embeddings import OpenAIEmbeddings\nfrom langchain.retrievers.document_compressors import EmbeddingsFilter\n\n# os.environ[\"OPENAI_API_KEY \"] = getpass()\nembdeddings_filter = EmbeddingsFilter(embeddings=embeddings)\ncompression_retriever_filter = ContextualCompressionRetriever(base_retriever=vector_retriever,\n                                                       base_compressor=embdeddings_filter)\n\n# compressed_docs = compression_retriever_filter.get_relevant_documents(query=\"How does Mistral outperform Llama?\")\n# pretty_print_docs(compressed_docs)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:16:50.919137Z","iopub.execute_input":"2024-06-29T08:16:50.919495Z","iopub.status.idle":"2024-06-29T08:16:50.924888Z","shell.execute_reply.started":"2024-06-29T08:16:50.919464Z","shell.execute_reply":"2024-06-29T08:16:50.924003Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"print(type(compression_retriever_filter))","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:16:54.069938Z","iopub.execute_input":"2024-06-29T08:16:54.070639Z","iopub.status.idle":"2024-06-29T08:16:54.075094Z","shell.execute_reply.started":"2024-06-29T08:16:54.070595Z","shell.execute_reply":"2024-06-29T08:16:54.074186Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"<class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'>\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:16:54.374498Z","iopub.execute_input":"2024-06-29T08:16:54.375329Z","iopub.status.idle":"2024-06-29T08:16:54.379521Z","shell.execute_reply.started":"2024-06-29T08:16:54.375298Z","shell.execute_reply":"2024-06-29T08:16:54.378586Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from langchain.chains import LLMChain\nfrom langchain.memory import ConversationBufferMemory\n# from langchain_experimental.chat_models import Llama2Chat","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:17:08.254093Z","iopub.execute_input":"2024-06-29T08:17:08.254702Z","iopub.status.idle":"2024-06-29T08:17:08.285210Z","shell.execute_reply.started":"2024-06-29T08:17:08.254672Z","shell.execute_reply":"2024-06-29T08:17:08.284418Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"from langchain_core.messages import SystemMessage\nfrom langchain_core.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    MessagesPlaceholder,\n)\n\ntemplate_messages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    HumanMessagePromptTemplate.from_template(\"{text}\"),\n]\nprompt_template = ChatPromptTemplate.from_messages(template_messages)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:13:29.264910Z","iopub.execute_input":"2024-06-29T04:13:29.265565Z","iopub.status.idle":"2024-06-29T04:13:29.271342Z","shell.execute_reply.started":"2024-06-29T04:13:29.265526Z","shell.execute_reply":"2024-06-29T04:13:29.270435Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\nchain = LLMChain(llm=chat, prompt=prompt_template, memory=memory)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:13:53.082306Z","iopub.execute_input":"2024-06-29T04:13:53.083111Z","iopub.status.idle":"2024-06-29T04:13:53.156493Z","shell.execute_reply.started":"2024-06-29T04:13:53.083077Z","shell.execute_reply":"2024-06-29T04:13:53.155558Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n  warn_deprecated(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The **Contextualize Prompt** reformulates user questions to be clear and standalone, ensuring queries are understood with prior chat history. The **System Prompt** guides the assistant to answer queries by instructing the llm to refer the information from the given context, and do not generate answers on its own.\n\nThe **ChatPromptTemplate** combines system instructions, chat history, and the latest user input into a structured prompt, helping the model understand and generate accurate responses.","metadata":{}},{"cell_type":"code","source":"CONTEXTUALIZE_PROMPT = \"\"\"Given a chat history and the latest user question which \\\nmight reference context in the chat history, formulate a standalone question which can be \\\nunderstood without the chat history. Do NOT answer the question, just reformulate it if needed \\\nand otherwise return it as is.\n\nChat History:\n\n{chat_history}\"\"\"\n\nSYSTEM_PROMPT2 = \"\"\"\nYou have to answer in the same language as the question. \\\nFirst determine in which language is the question.\n\nYou are a specialist in Artificial Intelligence. You have to assist the users with their\\\nquestion. You first have to search answers in the \"Knowledge Base\". If no answers are found \\\nin the \"Knowledge Base\", then reply \"Sorry, it seems I missed this in my training...\".\n\nKnowledge Base:\n\n{context}\n\nChat History:\n\n{chat_history}\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:14:06.990881Z","iopub.execute_input":"2024-06-29T04:14:06.991804Z","iopub.status.idle":"2024-06-29T04:14:06.997035Z","shell.execute_reply.started":"2024-06-29T04:14:06.991759Z","shell.execute_reply":"2024-06-29T04:14:06.996028Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"contextualize_q_system_prompt = (\n    \"Given a chat history and the latest user question \"\n    \"which might reference context in the chat history, \"\n    \"formulate a standalone question which can be understood \"\n    \"without the chat history. Do NOT answer the question, \"\n    \"just reformulate it if needed and otherwise return it as is.\"\n)\n\ncontextualize_q_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", contextualize_q_system_prompt),\n        MessagesPlaceholder(\"chat_history\"),\n        (\"human\", \"{input}\"),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:19:44.942535Z","iopub.execute_input":"2024-06-29T08:19:44.942902Z","iopub.status.idle":"2024-06-29T08:19:44.948524Z","shell.execute_reply.started":"2024-06-29T08:19:44.942872Z","shell.execute_reply":"2024-06-29T08:19:44.947585Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"**History-Aware Retriever** integrates previous chat history with the current query to improve the relevance of the retrieved documents, ensuring contextual awareness. This component improve the RAG system by ensuring the assistant is contextually aware and capable of providing precise and relevant answers, leading to a more coherent and helpful conversational experience.","metadata":{}},{"cell_type":"code","source":"history_aware_retriever = create_history_aware_retriever(\n    chat, vector_retriever, contextualize_q_prompt\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:20:12.621068Z","iopub.execute_input":"2024-06-29T08:20:12.621752Z","iopub.status.idle":"2024-06-29T08:20:12.626048Z","shell.execute_reply.started":"2024-06-29T08:20:12.621720Z","shell.execute_reply":"2024-06-29T08:20:12.625187Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"The **Question Answer Chain** uses the retrieved context and chat history to generate concise answers, ensuring responses are based on the most relevant information. ","metadata":{}},{"cell_type":"code","source":"system_prompt = (\n    \"You are an assistant for question-answering tasks. \"\n    \"Use the following pieces of retrieved context to answer \"\n    \"the question. If you don't know the answer, say that you \"\n    \"don't know. Use three sentences maximum and keep the \"\n    \"answer concise.\"\n    \"\\n\\n\"\n    \"{context}\"\n)\nqa_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        MessagesPlaceholder(\"chat_history\"),\n        (\"human\", \"{input}\"),\n    ]\n)\nquestion_answer_chain = create_stuff_documents_chain(chat, qa_prompt)\n\nrag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:22:22.108215Z","iopub.execute_input":"2024-06-29T08:22:22.108819Z","iopub.status.idle":"2024-06-29T08:22:22.115848Z","shell.execute_reply.started":"2024-06-29T08:22:22.108787Z","shell.execute_reply":"2024-06-29T08:22:22.114848Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"`ChatMessageHistory` and `BaseChatMessageHistory` manage the chat histories. The `get_session_history` function retrieves or creates a chat history for each session ID. The `RunnableWithMessageHistory` wraps the RAG chain, incorporating session-specific chat history into the process. ","metadata":{}},{"cell_type":"code","source":"from langchain_community.chat_message_histories import ChatMessageHistory\nfrom langchain_core.chat_history import BaseChatMessageHistory\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\n\nstore = {}\n\n\ndef get_session_history(session_id: str) -> BaseChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n\n\nconversational_rag_chain = RunnableWithMessageHistory(\n    rag_chain,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n    output_messages_key=\"answer\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:25:55.933417Z","iopub.execute_input":"2024-06-29T08:25:55.934140Z","iopub.status.idle":"2024-06-29T08:25:55.942934Z","shell.execute_reply.started":"2024-06-29T08:25:55.934109Z","shell.execute_reply":"2024-06-29T08:25:55.942062Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"conversational_rag_chain.invoke(\n    {\"input\": \"Tell me about GPT 4\"},\n    config={\n        \"configurable\": {\"session_id\": \"abc123\"}\n    },  # constructs a key \"abc123\" in `store`.\n)[\"answer\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:26:29.895393Z","iopub.execute_input":"2024-06-29T08:26:29.895750Z","iopub.status.idle":"2024-06-29T08:26:43.674867Z","shell.execute_reply.started":"2024-06-29T08:26:29.895721Z","shell.execute_reply":"2024-06-29T08:26:43.673993Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"\"GPT-4 is a language model created by OpenAI, which is a non-profit research company that develops and promotes the use of AI technologies to benefit humanity. It is the latest in a series of Generative Pre-trained Transformer (GPT) models, which are known for their ability to understand and generate human-like language. \\n\\nGPT-4 has several improvements and updates compared to its predecessor, GPT-3.5. Firstly, it is more reliable, creative, and able to handle much more nuanced instructions, which makes it particularly useful in language tasks that require adaptability and creativity. It has better performance in generating text that is more coherent, less repetitive, and contains less factual errors. Additionally, it excels in tasks involving common sense reasoning, command simulation, and question answering. \\n\\nOne of the most significant advancements in GPT-4 is its performance in multilingual tasks, where it outperforms GPT-3.5 in 24 out of 26 languages tested. This makes it a significant improvement over GPT-3.5 in language support, low-resource languages such as Latvian, Welsh, and Swahili. \\n\\nHowever, it's important to note that GPT-4, like all language models, has certain limitations and potential risks. It may sometimes make simple reasoning errors or be overly gullible in accepting obvious false statements. It can also generate harmful advice, buggy code, or inaccurate information. Therefore, it's crucial to use it responsibly and complement it with deployment-time safety techniques like monitoring for abuse. \\n\\nOverall, GPT-4 is an advanced language model that boasts impressive language capabilities and innovations, and has the potential to significantly influence society and drive many applications, from chatbots to supercharging text-based AI tools.  While it requires additional work and monitoring to ensure its safety and effectiveness, it remains an invaluable resource in the field of natural language processing and AI.\""},"metadata":{}}]},{"cell_type":"code","source":"conversational_rag_chain.invoke(\n    {\"input\": \"Tell me how it outperfroms other models\"},\n    config={\n        \"configurable\": {\"session_id\": \"abc123\"}\n    },  # constructs a key \"abc123\" in `store`.\n)[\"answer\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:27:07.237781Z","iopub.execute_input":"2024-06-29T08:27:07.238560Z","iopub.status.idle":"2024-06-29T08:27:34.719688Z","shell.execute_reply.started":"2024-06-29T08:27:07.238528Z","shell.execute_reply":"2024-06-29T08:27:34.718807Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"\"GPT-4 excels in a wide range of natural language processing tasks compared to other language models due to several key factors. Firstly, its capacity enables the model to capture long-range dependencies in sentences, allowing it to generate more coherent and contextually appropriate responses. This helps improve its understanding and generation of language in a variety of settings. \\n\\nSecondly, GPT-4 boasts impressive performance in low-resource languages. Unlike other models that struggle with low-resource languages due to the lack of extensive training data, GPT-4 excels even in these scenarios. This is because the model is pre-trained on a vast amount of data in multiple languages, enabling it to develop a general understanding of language patterns and semantics. \\n\\nAdditionally, GPT-4 performs better at generating text that is more coherent, less repetitive, and contains less factual errors. This is largely due to the model's Generative Adversarial Network (GAN) architecture, which has been refined and improved in GPT-4 compared to previous models. This improved GAN architecture enables the model to better capture the distribution of text, resulting in more natural and accurate generation. The fine-tuning process for GPT-4 has also been improved, which helps it outperform GPT-3.5.\\n\\nAll of these factors give it an edge over other large language models, making it more versatile, creative, and adaptable, and able to handle more nuanced instructions. However, it's important to note that while GPT-4 is an advanced model that has made significant advancements in language processing, there are still areas where it has limitations and potential risks, and it is always important to use such powerful tools responsibly with adequate safety measures in place.\""},"metadata":{}}]}]}